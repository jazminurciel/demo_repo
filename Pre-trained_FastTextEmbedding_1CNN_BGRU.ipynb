{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from flask import abort, Flask, jsonify, request\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import spacy\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\bq\\b|\\bk\\b', 'que', text) # replace q or x with que\n",
    "    text = re.sub(r'\\bd\\b', 'de', text) # replace d with de\n",
    "    text = re.sub(r'\\bx\\b', 'por', text) # replace x with por\n",
    "    text = re.sub(r'\\btmb\\b', 'también', text) # replace tmb with tambien\n",
    "\n",
    "    duplicates = re.compile(r'([^(c,l,n,r)L0])\\1{1,}')\n",
    "    double_clnr = re.compile(r\"(.)\\1{2,}\")\n",
    "    while duplicates.search(text)!=None:\n",
    "        text = text.replace(duplicates.search(text).group(),duplicates.search(text).group()[0]) #remove multiple letters\n",
    "    text = double_clnr.sub(r\"\\1\\1\", text) #except double c, l, n, and r\n",
    "\n",
    "    text = re.sub(r'([ja]{5,}|[je]{5,}|[ji]{5,}|[ha]{5,}|[he]{5,})', 'jaja', text)  # remove dirty laughs\n",
    "\n",
    "    text = re.sub(r'(\\.|,|:|;|!|\\?|\\[|\\]|\\(|\\))', ' ', text)  # replace simbols between words with spaces\n",
    "    text = re.sub(r'\\d+', '', text) #remove numbers\n",
    "\n",
    "    text = re.sub(r'[%s]' % re.escape(\"\"\"¿¡!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~…\"\"\"), '', text)  # remove punctuations\n",
    "    text = re.sub(r'\\b[^aeyou]\\b', ' ', text) # remove single char\n",
    "    text = re.sub('\\s+', ' ', text)  # remove extra whitespace\n",
    "\n",
    "    text = text.encode('latin', 'ignore').decode('latin')\n",
    "\n",
    "    max_edit_distance_lookup = 3\n",
    "    text = sym_spell.lookup_compound(text,max_edit_distance_lookup)[0].term\n",
    "\n",
    "    tokens= nlp(u\"\"+text)\n",
    "    new_text= ' '.join([t.lemma_ for t in tokens])\n",
    "\n",
    "    return [new_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "sym_spell = SymSpell(\n",
    "    max_dictionary_edit_distance=3,\n",
    "    prefix_length=7,\n",
    "    count_threshold=1,\n",
    "    compact_level=5,\n",
    ")\n",
    "\n",
    "sym_spell.load_dictionary(corpus='resources/es_real_freq_full.txt',term_index=0,count_index=1,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 12:12:47.375678  1580 deprecation_wrapper.py:119] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0829 12:12:47.624739  1580 deprecation_wrapper.py:119] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0829 12:12:47.750817  1580 deprecation_wrapper.py:119] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0829 12:12:47.770796  1580 deprecation_wrapper.py:119] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0829 12:12:47.989521  1580 deprecation_wrapper.py:119] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0829 12:12:47.996136  1580 deprecation.py:506] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0829 12:12:48.998857  1580 deprecation_wrapper.py:119] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0829 12:12:49.400892  1580 deprecation_wrapper.py:119] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0829 12:12:49.419909  1580 deprecation.py:323] From C:\\anaconda3\\envs\\benja\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model('NN_models/FastText_1CNN-BGRU_lemma.h5')\n",
    "tokenizer = pickle.load(open('models/tokenizer_24_embedding_fasttext_1cnn-bgru.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 24, 300)           18405300  \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 22, 128)           115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 32)                13920     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 18,534,821\n",
      "Trainable params: 129,521\n",
      "Non-trainable params: 18,405,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION SET\n",
      "Confusion Matrix:\n",
      "\n",
      "[[48  4]\n",
      " [16 31]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.750     0.923     0.828        52\n",
      "           P      0.886     0.660     0.756        47\n",
      "\n",
      "   micro avg      0.798     0.798     0.798        99\n",
      "   macro avg      0.818     0.791     0.792        99\n",
      "weighted avg      0.814     0.798     0.794        99\n",
      "\n",
      "\n",
      "Model Validation set Accuracy: 0.797979797979798\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST VALIDATION SET\n",
      "Confusion Matrix:\n",
      "\n",
      "[[1006  400]\n",
      " [ 180  943]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.848     0.716     0.776      1406\n",
      "           P      0.702     0.840     0.765      1123\n",
      "\n",
      "   micro avg      0.771     0.771     0.771      2529\n",
      "   macro avg      0.775     0.778     0.771      2529\n",
      "weighted avg      0.783     0.771     0.771      2529\n",
      "\n",
      "\n",
      "Test set Accuracy: 0.7706603400553579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_validation = pd.read_csv('validation_sets/validation_labeled.csv',encoding='utf-8-sig')\n",
    "df_test= pd.read_csv('test_sets/intertass_all_clean.csv',encoding='utf-8-sig')\n",
    "df_test = df_test[df_test['sentiments'].isin(['P','N'])]\n",
    "\n",
    "word_normalization = 'lemma_content_no'\n",
    "max_len=24\n",
    "\n",
    "X_validation,y_validation = df_validation[word_normalization],pd.Series([1 if i=='P' else 0 for i in df_validation['sentiment']],index = df_validation.index)\n",
    "X_validation_tokens = tokenizer.texts_to_sequences(X_validation.values)\n",
    "X_validation_pad = pad_sequences(X_validation_tokens,maxlen=max_len)\n",
    "\n",
    "X_test,y_test = df_test[word_normalization],pd.Series([1 if i=='P' else 0 for i in df_test['sentiments']],index = df_test.index)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test.values)\n",
    "X_test_pad =  pad_sequences(X_test_tokens,maxlen=max_len)\n",
    "\n",
    "pred1 = model.predict_classes(X_validation_pad)\n",
    "pred2 = model.predict_classes(X_test_pad)\n",
    "\n",
    "print(\"VALIDATION SET\")\n",
    "print(f\"Confusion Matrix:\\n\\n{confusion_matrix(y_validation,pred1)}\\n\")   \n",
    "print(classification_report(y_validation,pred1,labels=[0,1],target_names=['N','P'], digits = 3))  \n",
    "print(f\"\\nModel Validation set Accuracy: {accuracy_score(y_validation, pred1)}\\n\") \n",
    "\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"TEST VALIDATION SET\")\n",
    "print(f\"Confusion Matrix:\\n\\n{confusion_matrix(y_test,pred2)}\\n\")   \n",
    "print(classification_report(y_test,pred2,labels=[0,1],target_names=['N','P'],digits = 3))  \n",
    "print(f\"\\nTest set Accuracy: {accuracy_score(y_test, pred2)}\\n\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sesion id</th>\n",
       "      <th>actividad</th>\n",
       "      <th>TEXTO</th>\n",
       "      <th>clean_content_no</th>\n",
       "      <th>clean_content</th>\n",
       "      <th>lemma_content_no</th>\n",
       "      <th>lemma_content</th>\n",
       "      <th>stemm_content_no</th>\n",
       "      <th>stemm_content</th>\n",
       "      <th>Etiquetador 1 (AMG)</th>\n",
       "      <th>Etiquetador 2 (ICM)</th>\n",
       "      <th>Etiquetador 3 (JGR)</th>\n",
       "      <th>Etiquetador 4 (RECA)</th>\n",
       "      <th>Etiquetador 5 (JH)</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Model_score</th>\n",
       "      <th>Model_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1239</td>\n",
       "      <td>210</td>\n",
       "      <td>Problematica</td>\n",
       "      <td>nada, simplemente desepcionado de como.se.dan ...</td>\n",
       "      <td>nada simplemente decepcionado de como se dan l...</td>\n",
       "      <td>nada simplemente decepcionado</td>\n",
       "      <td>nadar simplemente decepcionar de comer se dar ...</td>\n",
       "      <td>nadar simplemente decepcionar</td>\n",
       "      <td>nad simplement decepcion de com se dan las cos</td>\n",
       "      <td>nad simplement decepcion</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.398659</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245</td>\n",
       "      <td>210</td>\n",
       "      <td>Explicacion</td>\n",
       "      <td>pues ver lo positivo de ello</td>\n",
       "      <td>pues ver lo positivo de ello</td>\n",
       "      <td>positivo</td>\n",
       "      <td>pues ver el positivo de él</td>\n",
       "      <td>positivo</td>\n",
       "      <td>pues ver lo posit de ello</td>\n",
       "      <td>posit</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>0.826883</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1248</td>\n",
       "      <td>210</td>\n",
       "      <td>Aprendizaje</td>\n",
       "      <td>darme un tiempo para refelxionar y no nada mas...</td>\n",
       "      <td>darme un tiempo para reflexionar y no nada mas...</td>\n",
       "      <td>darme tiempo reflexionar no nada irme precio p...</td>\n",
       "      <td>darme uno tiempo parir reflexionar y no nadar ...</td>\n",
       "      <td>darme tiempo reflexionar no nadar irme preciar...</td>\n",
       "      <td>darm un tiemp par reflexion y no nad mas irme ...</td>\n",
       "      <td>darm tiemp reflexion no nad irme preci pensamient</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>0.246394</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1329</td>\n",
       "      <td>226</td>\n",
       "      <td>Obstaculo_Actividad</td>\n",
       "      <td>planeo realizarla por la tarde del día</td>\n",
       "      <td>planeo realizara por la tarde del día</td>\n",
       "      <td>planeo realizara</td>\n",
       "      <td>planear realizar por lo tardar del día</td>\n",
       "      <td>planear realizar</td>\n",
       "      <td>plane realiz por la tard del dia</td>\n",
       "      <td>plane realiz</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>0.651749</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1345</td>\n",
       "      <td>255</td>\n",
       "      <td>Problematica</td>\n",
       "      <td>las personas a quienes atiendo me hacen enojar...</td>\n",
       "      <td>las personas a quienes atiendo me hacen enojar...</td>\n",
       "      <td>personas atiendo enojar no entienden instrucci...</td>\n",
       "      <td>los personar a quien atender me hacer enojar p...</td>\n",
       "      <td>personar atender enojar no entender instrucció...</td>\n",
       "      <td>las person a quien atiend me hac enoj porqu no...</td>\n",
       "      <td>person atiend enoj no entiend instruccion quier</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.243402</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  sesion id            actividad  \\\n",
       "0  1239        210         Problematica   \n",
       "1  1245        210          Explicacion   \n",
       "2  1248        210          Aprendizaje   \n",
       "3  1329        226  Obstaculo_Actividad   \n",
       "4  1345        255         Problematica   \n",
       "\n",
       "                                               TEXTO  \\\n",
       "0  nada, simplemente desepcionado de como.se.dan ...   \n",
       "1                       pues ver lo positivo de ello   \n",
       "2  darme un tiempo para refelxionar y no nada mas...   \n",
       "3             planeo realizarla por la tarde del día   \n",
       "4  las personas a quienes atiendo me hacen enojar...   \n",
       "\n",
       "                                    clean_content_no  \\\n",
       "0  nada simplemente decepcionado de como se dan l...   \n",
       "1                       pues ver lo positivo de ello   \n",
       "2  darme un tiempo para reflexionar y no nada mas...   \n",
       "3              planeo realizara por la tarde del día   \n",
       "4  las personas a quienes atiendo me hacen enojar...   \n",
       "\n",
       "                                       clean_content  \\\n",
       "0                      nada simplemente decepcionado   \n",
       "1                                           positivo   \n",
       "2  darme tiempo reflexionar no nada irme precio p...   \n",
       "3                                   planeo realizara   \n",
       "4  personas atiendo enojar no entienden instrucci...   \n",
       "\n",
       "                                    lemma_content_no  \\\n",
       "0  nadar simplemente decepcionar de comer se dar ...   \n",
       "1                         pues ver el positivo de él   \n",
       "2  darme uno tiempo parir reflexionar y no nadar ...   \n",
       "3             planear realizar por lo tardar del día   \n",
       "4  los personar a quien atender me hacer enojar p...   \n",
       "\n",
       "                                       lemma_content  \\\n",
       "0                      nadar simplemente decepcionar   \n",
       "1                                           positivo   \n",
       "2  darme tiempo reflexionar no nadar irme preciar...   \n",
       "3                                   planear realizar   \n",
       "4  personar atender enojar no entender instrucció...   \n",
       "\n",
       "                                    stemm_content_no  \\\n",
       "0     nad simplement decepcion de com se dan las cos   \n",
       "1                          pues ver lo posit de ello   \n",
       "2  darm un tiemp par reflexion y no nad mas irme ...   \n",
       "3                   plane realiz por la tard del dia   \n",
       "4  las person a quien atiend me hac enoj porqu no...   \n",
       "\n",
       "                                       stemm_content Etiquetador 1 (AMG)  \\\n",
       "0                           nad simplement decepcion                   N   \n",
       "1                                              posit                   N   \n",
       "2  darm tiemp reflexion no nad irme preci pensamient                   P   \n",
       "3                                       plane realiz                   P   \n",
       "4    person atiend enoj no entiend instruccion quier                   N   \n",
       "\n",
       "  Etiquetador 2 (ICM) Etiquetador 3 (JGR) Etiquetador 4 (RECA)  \\\n",
       "0                   N                   N                    N   \n",
       "1                   P                   P                    P   \n",
       "2                   P                   P                    P   \n",
       "3                   P                   N                    N   \n",
       "4                   N                   N                    N   \n",
       "\n",
       "  Etiquetador 5 (JH) sentiment  Model_score Model_value  \n",
       "0                  N         N     0.398659           N  \n",
       "1                  P         P     0.826883          P+  \n",
       "2                  P         P     0.246394           N  \n",
       "3                  P         P     0.651749           P  \n",
       "4                  N         N     0.243402           N  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation['Model_score'] = model.predict(X_validation_pad)\n",
    "df_validation['Model_value'] = ['P+' if t > 0.80 else \n",
    "                                'P' if 0.80>t>0.60 else \n",
    "                                'NEU' if 0.60>t>0.40 else\n",
    "                                'N' if 0.40>t>0.20 else\n",
    "                                'N+' for t in df_validation['Model_score']]\n",
    "\n",
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text: no me gusto el cuarto\n",
      "\n",
      "Text after preprocessing: no me gustar el cuartar\n",
      "\n",
      "Score 0.20955899357795715\n",
      "Event: N\n",
      "\n",
      "Process took 0.3620619773864746 seconds to finish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"no me gusto el cuarto\n",
    "\"\"\"\n",
    "\n",
    "start = time()\n",
    "print(f\"\\nInput text: {text}\")\n",
    "clean = clean_text(text)\n",
    "print(f\"Text after preprocessing: {clean[0]}\\n\")\n",
    "text_tokens = tokenizer.texts_to_sequences(clean)\n",
    "text_pad = pad_sequences(text_tokens,maxlen=max_len)\n",
    "score = model.predict(text_pad)[0][0]\n",
    "print(f\"Score {score}\")\n",
    "event = (\"P+\"if score>=0.8 else\n",
    "         \"P\" if 0.60<=score<0.8 else\n",
    "         \"NEU\" if 0.40<=score<0.60 else\n",
    "         \"N\" if 0.20<=score<0.40 else\n",
    "         \"N+\")\n",
    "print(f\"Event: {event}\")\n",
    "print(f\"\\nProcess took {time()-start} seconds to finish\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:benja]",
   "language": "python",
   "name": "conda-env-benja-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
