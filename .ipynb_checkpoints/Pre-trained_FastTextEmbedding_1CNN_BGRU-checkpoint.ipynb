{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from flask import abort, Flask, jsonify, request\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import spacy\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\bq\\b|\\bk\\b', 'que', text) # replace q or x with que\n",
    "    text = re.sub(r'\\bd\\b', 'de', text) # replace d with de\n",
    "    text = re.sub(r'\\bx\\b', 'por', text) # replace x with por\n",
    "    text = re.sub(r'\\btmb\\b', 'también', text) # replace tmb with tambien\n",
    "\n",
    "    duplicates = re.compile(r'([^(c,l,n,r)L0])\\1{1,}')\n",
    "    double_clnr = re.compile(r\"(.)\\1{2,}\")\n",
    "    while duplicates.search(text)!=None:\n",
    "        text = text.replace(duplicates.search(text).group(),duplicates.search(text).group()[0]) #remove multiple letters\n",
    "    text = double_clnr.sub(r\"\\1\\1\", text) #except double c, l, n, and r\n",
    "\n",
    "    text = re.sub(r'([ja]{5,}|[je]{5,}|[ji]{5,}|[ha]{5,}|[he]{5,})', 'jaja', text)  # remove dirty laughs\n",
    "\n",
    "    text = re.sub(r'(\\.|,|:|;|!|\\?|\\[|\\]|\\(|\\))', ' ', text)  # replace simbols between words with spaces\n",
    "    text = re.sub(r'\\d+', '', text) #remove numbers\n",
    "\n",
    "    text = re.sub(r'[%s]' % re.escape(\"\"\"¿¡!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~…\"\"\"), '', text)  # remove punctuations\n",
    "    text = re.sub(r'\\b[^aeyou]\\b', ' ', text) # remove single char\n",
    "    text = re.sub('\\s+', ' ', text)  # remove extra whitespace\n",
    "\n",
    "    text = text.encode('latin', 'ignore').decode('latin')\n",
    "\n",
    "    max_edit_distance_lookup = 3\n",
    "    text = sym_spell.lookup_compound(text,max_edit_distance_lookup)[0].term\n",
    "\n",
    "    tokens= nlp(u\"\"+text)\n",
    "    new_text= ' '.join([t.lemma_ for t in tokens])\n",
    "\n",
    "    return [new_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "sym_spell = SymSpell(\n",
    "    max_dictionary_edit_distance=3,\n",
    "    prefix_length=7,\n",
    "    count_threshold=1,\n",
    "    compact_level=5,\n",
    ")\n",
    "\n",
    "sym_spell.load_dictionary(corpus='resources/es_real_freq_full.txt',term_index=0,count_index=1,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('NN_models/FastText_1CNN-BGRU_lemma.h5')\n",
    "tokenizer = pickle.load(open('models/tokenizer_24_embedding_fasttext_1cnn-bgru.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.read_csv('test_sets/validation_labeled.csv',encoding='utf-8-sig')\n",
    "df_test= pd.read_csv('test_sets/intertass_all_clean.csv',encoding='utf-8-sig')\n",
    "df_test = df_test[df_test['sentiments'].isin(['P','N'])]\n",
    "\n",
    "word_normalization = 'lemma_content_no'\n",
    "max_len=24\n",
    "\n",
    "X_validation,y_validation = df_validation[word_normalization],pd.Series([1 if i=='P' else 0 for i in df_validation['sentiment']],index = df_validation.index)\n",
    "X_validation_tokens = tokenizer.texts_to_sequences(X_validation.values)\n",
    "X_validation_pad = pad_sequences(X_validation_tokens,maxlen=max_len)\n",
    "\n",
    "X_test,y_test = df_test[word_normalization],pd.Series([1 if i=='P' else 0 for i in df_test['sentiments']],index = df_test.index)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test.values)\n",
    "X_test_pad =  pad_sequences(X_test_tokens,maxlen=max_len)\n",
    "\n",
    "pred1 = model.predict_classes(X_validation_pad)\n",
    "pred2 = model.predict_classes(X_test_pad)\n",
    "\n",
    "print(\"VALIDATION SET\")\n",
    "print(f\"Confusion Matrix:\\n\\n{confusion_matrix(y_validation,pred1)}\\n\")   \n",
    "print(classification_report(y_validation,pred1,labels=[0,1],target_names=['N','P'], digits = 3))  \n",
    "print(f\"\\nModel Validation set Accuracy: {accuracy_score(y_validation, pred1)}\\n\") \n",
    "\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"TEST VALIDATION SET\")\n",
    "print(f\"Confusion Matrix:\\n\\n{confusion_matrix(y_test,pred2)}\\n\")   \n",
    "print(classification_report(y_test,pred2,labels=[0,1],target_names=['N','P'],digits = 3))  \n",
    "print(f\"\\nTest set Accuracy: {accuracy_score(y_test, pred2)}\\n\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation['Model_score'] = model.predict(X_validation_pad)\n",
    "df_validation['Model_value'] = ['P+' if t > 0.80 else \n",
    "                                'P' if 0.80>t>0.60 else \n",
    "                                'NEU' if 0.60>t>0.40 else\n",
    "                                'N' if 0.40>t>0.20 else\n",
    "                                'N+' for t in df_validation['Model_score']]\n",
    "\n",
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Soy una persona valiosa y especial. Debo aprender a quererme más\n",
    "\"\"\"\n",
    "\n",
    "start = time()\n",
    "print(f\"\\nInput text: {text}\")\n",
    "clean = clean_text(text)\n",
    "print(f\"Text after preprocessing: {clean[0]}\\n\")\n",
    "text_tokens = tokenizer.texts_to_sequences(clean)\n",
    "text_pad = pad_sequences(text_tokens,maxlen=max_len)\n",
    "score = model.predict(text_pad)[0][0]\n",
    "print(f\"Score {score}\")\n",
    "event = (\"P+\"if score>=0.8 else\n",
    "         \"P\" if 0.60<=score<0.8 else\n",
    "         \"NEU\" if 0.40<=score<0.60 else\n",
    "         \"N\" if 0.20<=score<0.40 else\n",
    "         \"N+\")\n",
    "print(f\"Event: {event}\")\n",
    "print(f\"\\nProcess took {time()-start} seconds to finish\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
